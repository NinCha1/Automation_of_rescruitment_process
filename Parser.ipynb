{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "import time, csv\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.error import HTTPError, URLError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib import  request\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://hh.ru/resumes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('chromedriver',options = chrome_options)\n",
    "browser.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_resumes = '/Users/nincha/Desktop/Results/hh.csv'\n",
    "if os.path.exists(path_to_resumes) == False:\n",
    "        with open('/Users/nincha/Desktop/Results/hh.csv','a',encoding='utf8') as f:\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerow(('Title', 'IsEnglish', 'SpecCat', 'ExpPeriod', 'Salary', 'Age', ' Gender', \n",
    "                                      'City', 'WorkType', 'WorkSchedule', \n",
    "                                      'N_places', 'LastPlace', \n",
    "                                      'LastPlace2', 'LastPos', 'LastWorkDesc', 'LastPos2', 'LastWorkDesc2', 'Top 10 work', 'N_changeWork', 'N_Langs', 'RussianNative', 'LevelEnglish', \n",
    "                                      'Ready_2_move', 'Ready_4_business_trip', 'Description', 'Python', 'SQL', 'Power BI',\n",
    "                                      'EduLevel', 'N_Unis', \n",
    "                                      'LastUni', 'LastUni2', 'Top 10 Uni', 'URL', 'lastUniFaculty', 'lastUni2Faculty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_urls = '/Users/nincha/Desktop/Results/urls.csv'\n",
    "if os.path.exists(path_to_urls) == False:\n",
    "    with open('/Users/nincha/Desktop/Results/urls.csv', 'a', encoding='utf8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(data):\n",
    "    with open(path_to_resumes,'a',encoding='utf8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow((data['Title'],\n",
    "                         data['Is English'],\n",
    "                         data['SpecCat'],\n",
    "                         data['ExpPeriod'],\n",
    "                         data['Salary'],                         \n",
    "                         data['Age'],\n",
    "                         data['Gender'],\n",
    "                         data['City'],\n",
    "                         data['WorkType'], \n",
    "                         data['WorkSchedule'],\n",
    "                         data['N_places'], \n",
    "                         data['LastPlace'], \n",
    "                         data['LastPlace2'],\n",
    "                         data['LastPos'],\n",
    "                         data['LastWorkDesc'],\n",
    "                         data['LastPos2'],\n",
    "                         data['LastWorkDesc2'],\n",
    "                         data['Top 10 work'],\n",
    "                         data['N_changeWork'],\n",
    "                         data['N_Langs'],\n",
    "                         data['RussianNative'],\n",
    "                         data['LevelEnglish'],\n",
    "                         data['Ready_2_move'], \n",
    "                         data['Ready_4_business_trip'], \n",
    "                         data['Description'], \n",
    "                         data['Python'],\n",
    "                         data['SQL'],\n",
    "                         data['Power BI'],\n",
    "                         data['EduLevel'],\n",
    "                         data['N_Unis'],\n",
    "                         data['LastUni'],\n",
    "                         data['LastUni2'],\n",
    "                         data['Top 10 Uni'],\n",
    "                         data['URL'],\n",
    "                         data['lastUniFaculty'],\n",
    "                         data['lastUni2Faculty']\n",
    "                         )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_urls(data):\n",
    "  with open(path_to_urls, 'a', encoding='utf8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow([data['URL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv(path_to_urls)\n",
    "resumes_df = pd.read_csv(path_to_resumes, low_memory=False)\n",
    "existing_urls = urls_df['URL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumes_url_get(browser):    \n",
    "    #—Å–±–æ—Ä —Å—Ç—Ä–∞–Ω–∏—á–µ–∫ —Å —Ä–µ–∑—é–º–µ\n",
    "    resumes = browser.find_elements(By.CLASS_NAME, \"resume-search-item__content-wrapper\")     \n",
    "    for resume in resumes:\n",
    "        url = resume.find_element(By.CLASS_NAME, 'serp-item__title').get_attribute('href')\n",
    "        if url not in existing_urls:\n",
    "            csv_urls({'URL': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_resumes(url_, page):\n",
    "  url = url_ + '?page=' + str(page)\n",
    "  browser.get (url)\n",
    "  resumes_url_get(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profession = [\n",
    "    'analitik-bi', \n",
    "    'sistemnyy_analitik',\n",
    "    'analitik_prodazh',\n",
    "    'finansovyy_analitik',\n",
    "    'analitik-dannyh',\n",
    "    'data-analyst',\n",
    "    'biznes_analitik'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_pages(profession):\n",
    "    url = base_url + profession + '?customDomain=1'\n",
    "    browser.get(url)\n",
    "    pages = browser.find_elements(By.CLASS_NAME, 'pager-item-not-in-short-range')\n",
    "    return pages[-1].text, url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prof in profession:\n",
    "   page, url = number_of_pages(prof)\n",
    "   print(prof)\n",
    "   for i in tqdm(range(int(page))):\n",
    "    get_all_resumes(url, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Title' - –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏ -  ‚úÖ\n",
    "\n",
    "\n",
    "'SpecCat' - —Å–ø—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚úÖ\n",
    "\n",
    "\n",
    "'ExpPeriod' - ‚úÖ\n",
    "\n",
    "'Salary' - –æ–∂–∏–¥–∞–µ–º–∞—è –∑–ø ‚úÖ\n",
    "\n",
    "\n",
    "'Age' - –≤–æ–∑—Ä–∞—Å—Ç ‚úÖ\n",
    "\n",
    "\n",
    "'Gender' - –ø–æ–ª ‚úÖ\n",
    "\n",
    "\n",
    "'City' - –≥–æ—Ä–æ–¥  ‚úÖ\n",
    "\n",
    "\n",
    "'WorkType' - —Ç–∏–ø —Ä–∞–±–æ—Ç—ã ‚úÖ\n",
    "\n",
    "'WorkSchedule' - –≥—Ä–∞—Ñ–∏–∫ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ ‚úÖ\n",
    "\n",
    "'N_places' - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç —Ä–∞–±–æ—Ç—ã ‚úÖ\n",
    "\n",
    "'LastPlace' - –ø–æ—Å–ª–µ–¥–Ω–µ–µ –º–µ—Å—Ç–æ —Ä–∞–±–æ—Ç—ã 1 ‚úÖ\n",
    "\n",
    "'LastPlace_2' - –ø–æ—Å–ª–µ–¥–Ω–µ–µ –º–µ—Å—Ç–æ —Ä–∞–±–æ—Ç—ã 2 ‚úÖ\n",
    "\n",
    "'LastPos' - –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º –º–µ—Å—Ç–µ —Ä–∞–±–æ—Ç—ã 1 ‚úÖ\n",
    "\n",
    "'LastPos_2' - –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º –º–µ—Å—Ç–µ —Ä–∞–±–æ—Ç—ã 2 ‚úÖ\n",
    "\n",
    "'N_langs' - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ –≤–ª–∞–¥–µ–Ω–∏—è  ‚úÖ\n",
    "\n",
    "'Langs' - —Å–ø–∏—Å–æ–∫ —è–∑—ã–∫–æ–≤ ‚úÖ\n",
    "\n",
    "'Citizenship' - –≥—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ ‚úÖ\n",
    "\n",
    "'Time2Work' - –≤—Ä–µ–º—è –¥–æ —Ä–∞–±–æ—Ç—ã ‚úÖ\n",
    "\n",
    "'Ready_2_move' - —Ñ–ª–∞–≥ ‚Äò–≥–æ—Ç–æ–≤ –∫ –ø–µ—Ä–µ–µ–∑–¥—É‚Äô ‚úÖ\n",
    "\n",
    "'Ready_4_business_trip' -¬† —Ñ–ª–∞–≥ ‚Äò–≥–æ—Ç–æ–≤ –∫ –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞–º‚Äô ‚úÖ\n",
    "\n",
    "'Description' - –ª–∏—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ ‚úÖ\n",
    "\n",
    "'Tags' - —Ç–µ–≥–∏ (–æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ –∏–Ω—Ç–µ—Ä–µ—Å—ã) ‚úÖ\n",
    "\n",
    "'Graduation' - –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ üí¢\n",
    "\n",
    "'EduLevel' - —É—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ‚úÖ\n",
    "\n",
    "'LastUni' - –ø–æ—Å–ª–µ–¥–Ω–∏–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç 1 ‚úÖ\n",
    "\n",
    "'LastUni_2' - –ø–æ—Å–ª–µ–¥–Ω–∏–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç 2 ‚úÖ\n",
    "\n",
    "'N_Unis' - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç, –≥–¥–µ —á–µ–ª–æ–≤–µ–∫ –æ–±—É—á–∞–ª—Å—è ‚úÖ\n",
    "\n",
    "'URL'  ‚úÖ\n",
    "\n",
    "'LastUniFaculty' ‚úÖ\n",
    "\n",
    "'LastUni2Faculty' ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/nincha/Desktop/Results/hh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_parsing(url='https://hh.ru/resume/f651db6400090a6dc00039ed1f397176516f46?query=%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D1%82%D0%B8%D0%BA+BI&source=search&hhtmFrom=resumes_catalog'):\n",
    "\n",
    "  user_agent = ua.random\n",
    "    \n",
    "  try:\n",
    "      req = Request(\n",
    "        url, \n",
    "        data=None, \n",
    "        headers={'User-Agent': user_agent}\n",
    "      )\n",
    "      html = urlopen(req).read()\n",
    "  except HTTPError as e:\n",
    "      print('HTTPError: {}'.format(e.code))\n",
    "  except URLError as e:\n",
    "        print('URLError: {}'.format(e.reason))\n",
    "  else:\n",
    "\n",
    "  # request_site = Request(url, headers = hdr)\n",
    "  # html = urlopen(request_site).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # position\n",
    "    try:\n",
    "      title = soup.find(attrs={'class': 'resume-block__title-text', 'data-qa': 'resume-block-title-position'}).text #position\n",
    "    except:\n",
    "      title = ''\n",
    "\n",
    "    # specialization\n",
    "    try:\n",
    "      specCat = ''\n",
    "      specialities = soup.findAll(class_='resume-block__specialization')\n",
    "      for s in specialities:\n",
    "        specCat += s.text + \"\\n\"\n",
    "    except:\n",
    "      specCat = ''\n",
    "  \n",
    "    # WorkType and WorkSchedule\n",
    "    try:\n",
    "      block_position = soup.find(attrs = {'data-qa': 'resume-block-position'})\n",
    "      position_items = block_position.find(class_='resume-block-item-gap')\n",
    "    \n",
    "      work_position_array = position_items.findAll('p')\n",
    "\n",
    "      work_type = worktype_encoding(work_position_array[0].text)\n",
    "\n",
    "      work_schedule = workschedule_encoding(work_position_array[1].text)\n",
    "    \n",
    "    except:\n",
    "      work_type = ''\n",
    "      work_schedule = ''\n",
    "\n",
    "  \n",
    "    # experience\n",
    "    try:\n",
    "      expPeriod = []\n",
    "      experienceBlock = soup.find(attrs={'data-qa': 'resume-block-experience'})\n",
    "      expSpan = experienceBlock(class_='resume-block__title-text resume-block__title-text_sub')\n",
    "      exptime = expSpan[0].findChildren('span')\n",
    "      for i in exptime:\n",
    "        expPeriod.append(i.text.replace(u'\\xa0', u' '))\n",
    "      \n",
    "      expPeriod = formatted_date(expPeriod)\n",
    "    \n",
    "    except:\n",
    "      expPeriod = ''\n",
    "\n",
    "\n",
    "    # salary\n",
    "    try:\n",
    "      salary = soup.find(class_='resume-block__salary resume-block__title-text_salary').text\n",
    "      salary = salary.replace(u'\\u2009', u' ')\n",
    "      salary = salary.replace(u'\\xa0', u' ')\n",
    "      salary = price_to_RUB(salary)\n",
    "    except:\n",
    "      salary = ''\n",
    "\n",
    "    # age\n",
    "    try:\n",
    "      age = int(re.findall(r'\\d+', soup.find(attrs={'data-qa': 'resume-personal-age'}).text)[0])\n",
    "    except:\n",
    "      age = ''\n",
    "\n",
    "    # gender \n",
    "    try:\n",
    "      gender = gender_encoding(soup.find(attrs={'data-qa': 'resume-personal-gender'}).text)\n",
    "    except:\n",
    "      gender = ''\n",
    "  \n",
    "    # city\n",
    "    try:\n",
    "      city = soup.find(attrs={'data-qa': 'resume-personal-address'}).text\n",
    "    except:\n",
    "      city = ''\n",
    "\n",
    "    # N_langs, Langs\n",
    "    try:\n",
    "      language = []\n",
    "      languages = soup.findAll(attrs={'data-qa': 'resume-block-language-item'})\n",
    "      n_langs = len(languages)\n",
    "      for l in languages:\n",
    "        language.append(l.text)\n",
    "      native_russian, english_level = language_qualification(language)\n",
    "    except:\n",
    "      n_langs = ''\n",
    "      native_russian = ''\n",
    "      english_level = ''\n",
    "\n",
    "\n",
    "    # Work Experience\n",
    "    try:\n",
    "      work_experience_block = soup.find(attrs={'data-qa': 'resume-block-experience'})\n",
    "      works_description_block = work_experience_block.find(class_='resume-block-item-gap')\n",
    "\n",
    "      works_infos = works_description_block.findAll(class_='resume-block-item-gap')\n",
    "\n",
    "      time_spans = []\n",
    "\n",
    "      for work_info in works_infos:\n",
    "        time_spans.append(work_info.find(class_='bloko-column bloko-column_xs-4 bloko-column_s-2 bloko-column_m-2 bloko-column_l-2'))\n",
    "\n",
    "      \n",
    "      n_change = n_place_last_year(time_spans)\n",
    "      \n",
    "      lastPlace = works_infos[0].find(class_='bloko-text bloko-text_strong').text\n",
    "      work_top_10_flag = check_top_10(lastPlace, jobs)\n",
    "\n",
    "      lastExpTime = []\n",
    "\n",
    "      lastWorkTime = expSpan[0].findChildren('span')\n",
    "      for i in lastWorkTime:\n",
    "        lastExpTime.append(i.text.replace(u'\\xa0', u' '))\n",
    "      \n",
    "\n",
    "      lastPlace2 = ' '\n",
    "\n",
    "      lastPos = works_infos[0].find(attrs={'data-qa': 'resume-block-experience-position'}).text\n",
    "      lastWorkDesc = works_infos[0].find(attrs= {'data-qa': 'resume-block-experience-description'}).text\n",
    "      lastPos2 = ' '\n",
    "      lastWorkDesc2 = ' '\n",
    "\n",
    "      if len(works_infos) > 1:\n",
    "        lastPos2 = works_infos[1].find(attrs={'data-qa': 'resume-block-experience-position'}).text\n",
    "        lastPlace2 = works_infos[1].find(class_='bloko-text bloko-text_strong').text\n",
    "        lastWorkDesc2 = works_infos[1].find(attrs= {'data-qa': 'resume-block-experience-description'}).text\n",
    "\n",
    "      n_places = len(works_infos)\n",
    "\n",
    "    \n",
    "\n",
    "    except:\n",
    "      lastPos = ''\n",
    "      lastPos2 = ''\n",
    "      lastPlace = ''\n",
    "      lastPlace2 = ''\n",
    "      n_places = ''\n",
    "      n_change = ''\n",
    "      lastWorkDesc = ''\n",
    "      lastWorkDesc2 = ''\n",
    "      work_top_10_flag = ''\n",
    "\n",
    "    # About me\n",
    "    try:\n",
    "      about_me_block = soup.find(attrs={'data-qa': 'resume-block-skills'})\n",
    "      description = about_me_block.find(attrs={'data-qa': 'resume-block-skills-content'}).text\n",
    "    except:\n",
    "      description = ''\n",
    "    \n",
    "    # Skills Table\n",
    "    try:\n",
    "      skills = []\n",
    "      skills_table_block = soup.find(attrs = {'data-qa': 'skills-table'})\n",
    "      skills_container =  skills_table_block.find(class_='resume-block-item-gap')\n",
    "\n",
    "      skills_array = skills_container.findAll(class_='bloko-tag bloko-tag_inline bloko-tag_countable')\n",
    "\n",
    "      for skill in skills_array:\n",
    "        skills.append(skill.text)\n",
    "      \n",
    "      if 'Python' in skills:\n",
    "        python_flag = 1\n",
    "      else:\n",
    "        python_flag = 0\n",
    "      \n",
    "      if 'SQL' in skills:\n",
    "        sql_flag = 1\n",
    "      else:\n",
    "        sql_flag = 0\n",
    "      \n",
    "      if 'Power BI' in skills:\n",
    "        powerbi_flag = 1\n",
    "      else:\n",
    "        powerbi_flag = 0\n",
    "\n",
    "    except:\n",
    "      skills = ''\n",
    "      python_flag = ''\n",
    "      sql_flag = ''\n",
    "      powerbi_flag = ''\n",
    "\n",
    "\n",
    "\n",
    "      # Education\n",
    "    try:\n",
    "        education_block = soup.find(attrs={'data-qa': 'resume-block-education'})\n",
    "\n",
    "        edu_level = education_block.find(class_='resume-block-container').text\n",
    "        \n",
    "        edu_description_block = education_block.find(class_='resume-block-item-gap')\n",
    "\n",
    "        edu_infos = edu_description_block.findAll(class_ = 'resume-block-item-gap')\n",
    "\n",
    "        n_unis = len(edu_infos)\n",
    "\n",
    "        lastUni = edu_infos[0].find(attrs = {'data-qa': 'resume-block-education-name', 'class': 'bloko-text bloko-text_strong'}).text\n",
    "        lastUniFaculty = edu_infos[0].find(attrs = {'data-qa': 'resume-block-education-organization'}).text\n",
    "        lastUni2 = ' '\n",
    "        lastUni2Faculty = ' '\n",
    "      \n",
    "        \n",
    "        if n_unis > 1:\n",
    "          lastUni2 = edu_infos[1].find(attrs = {'data-qa': 'resume-block-education-name', 'class': 'bloko-text bloko-text_strong'}).text\n",
    "          lastUni2Faculty = edu_infos[1].find(attrs = {'data-qa': 'resume-block-education-organization'}).text\n",
    "\n",
    "        uni_top_10_flag = check_top_10(lastUni, universities) or check_top_10(lastUni2, universities)\n",
    "\n",
    "          \n",
    "    except:\n",
    "        edu_level = ''\n",
    "        n_unis = ''\n",
    "        lastUni = ''\n",
    "        lastUni2 = ''\n",
    "        uni_top_10_flag = ''\n",
    "        lastUniFaculty = ''\n",
    "        lastUni2Faculty = ''\n",
    "\n",
    "    # Ready To\n",
    "    try:\n",
    "      ready_to_string = soup.find(class_='bloko-translate-guard').text\n",
    "      \n",
    "\n",
    "      is_english_flag = is_english(ready_to_string)\n",
    "\n",
    "      ready_to_move = ' '\n",
    "      ready_business_trip = ' '\n",
    "\n",
    "      \n",
    "      if '–Ω–µ –≥–æ—Ç–æ–≤ –∫ –ø–µ—Ä–µ–µ–∑–¥—É' in ready_to_string or 'not willing to relocate' in ready_to_string or '–Ω–µ –≥–æ—Ç–æ–≤–∞ –∫ –ø–µ—Ä–µ–µ–∑–¥—É' in ready_to_string:\n",
    "        ready_to_move = False\n",
    "      else:\n",
    "        ready_to_move = True\n",
    "      \n",
    "      if 'not prepared for business trips' in ready_to_string or '–Ω–µ –≥–æ—Ç–æ–≤ –∫ –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞–º' in ready_to_string or '–Ω–µ –≥–æ—Ç–æ–≤–∞ –∫ –∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞–º' in ready_to_string:\n",
    "        ready_business_trip = False\n",
    "      else:\n",
    "        ready_business_trip = True\n",
    "      \n",
    "    except:\n",
    "      ready_to_move = ''\n",
    "      ready_business_trip = ''\n",
    "      is_english_flag = ''\n",
    "\n",
    "\n",
    "    # Additional Info\n",
    "    try:\n",
    "      addit_block = soup.find(attrs={'data-qa': 'resume-block-additional'})\n",
    "      all_add_info = addit_block.find(class_='resume-block-item-gap')\n",
    "      addit_array = all_add_info.findAll('p')\n",
    "\n",
    "      \n",
    "      citizenship = addit_array[0].text\n",
    "      citizenship = citizenship.lstrip('–ì—Ä–∞–∂–¥–∞–Ω—Å—Ç–≤–æ: ')\n",
    "      citizenship = citizenship.lstrip('Citizenship: ')\n",
    "\n",
    "      time_to_work = addit_array[-1].text\n",
    "      time_to_work = time_to_work.lstrip('Desired travel time to work: ')\n",
    "      time_to_work = time_to_work.lstrip('–ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤ –ø—É—Ç–∏ –¥–æ —Ä–∞–±–æ—Ç—ã: ')\n",
    "    except:\n",
    "      citizenship = ''\n",
    "      time_to_work = ''\n",
    "\n",
    "    url = url\n",
    "\n",
    "    data = {'Title': title,\n",
    "            'Is English': is_english_flag,\n",
    "            'SpecCat': specCat,\n",
    "            'ExpPeriod': expPeriod,\n",
    "            'Salary': salary,\n",
    "            'Age': age,\n",
    "            'Gender': gender,\n",
    "            'City': city,\n",
    "            'WorkType': work_type,\n",
    "            'WorkSchedule': work_schedule,\n",
    "            'N_places': n_places,\n",
    "            'LastPlace': lastPlace,\n",
    "            'LastPlace2': lastPlace2,\n",
    "            'LastWorkDesc': lastWorkDesc,\n",
    "            'LastPos': lastPos,\n",
    "            'LastPos2': lastPos2,\n",
    "            'LastWorkDesc2': lastWorkDesc2,\n",
    "            'Top 10 work': work_top_10_flag,\n",
    "            'N_changeWork': n_change,\n",
    "            'N_Langs': n_langs,\n",
    "            'RussianNative': native_russian,\n",
    "            'LevelEnglish': english_level,\n",
    "            'Ready_2_move': ready_to_move,\n",
    "            'Ready_4_business_trip': ready_business_trip,\n",
    "            'Description': description,\n",
    "            'Python': python_flag,\n",
    "            'SQL': sql_flag,\n",
    "            'Power BI': powerbi_flag,\n",
    "            'EduLevel': edu_level,\n",
    "            'N_Unis': n_unis,\n",
    "            'LastUni': lastUni,\n",
    "            'LastUni2': lastUni2,\n",
    "            'Top 10 Uni': uni_top_10_flag,\n",
    "            'URL': url,\n",
    "            'lastUniFaculty': lastUniFaculty,\n",
    "            'lastUni2Faculty': lastUni2Faculty\n",
    "            } \n",
    "    write_csv(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv(path_to_urls)\n",
    "urls = urls_df['URL'].values\n",
    "existing_data = resumes_df['URL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in tqdm(urls):\n",
    "  if url not in existing_data:\n",
    "    resume_parsing(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
